{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # Don't show debug info while training\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datetime\n",
    "from tensorflow.keras import layers, models\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Dense, Dropout, Input, Conv1D, LSTM, Dense, Flatten, Bidirectional, BatchNormalization, Activation, LayerNormalization, Concatenate, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from tensorflow.keras.layers import Add, Conv1D, MaxPooling1D, Dense, Dropout, BatchNormalization, GRU, LayerNormalization, ReLU, Input, Conv1D, LSTM, TimeDistributed, Dense, Activation, MultiHeadAttention, Attention, Bidirectional, RepeatVector, Masking, Multiply, Layer, Conv1DTranspose, Concatenate, ZeroPadding1D, Cropping1D, Lambda\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "import sys\n",
    "import warnings\n",
    "import json\n",
    "import optuna\n",
    "import random\n",
    "import matplotlib.gridspec as gridspec\n",
    "import os\n",
    "from tensorflow.keras.models import load_model\n",
    "import shap\n",
    "from itertools import product\n",
    "from tqdm import tqdm\n",
    "import umap\n",
    "from sklearn.cluster import AgglomerativeClustering, KMeans, DBSCAN\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE, Isomap, MDS, SpectralEmbedding\n",
    "from sklearn.decomposition import PCA, FactorAnalysis,  DictionaryLearning, NMF\n",
    "from Bio import motifs\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from Bio import motifs\n",
    "from Bio.Seq import Seq\n",
    "import h5py\n",
    "from modisco import tfmodisco_workflow\n",
    "import subprocess\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Validate the input data and custom variables to make sure the file exists, that the input columns exist, that the sequences and groups are in the right format, that all other custom variables are the right type and that sequences are not too long and there are not too many different groups.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_config_and_data(config):\n",
    "    \n",
    "    # Ensure 'full_file' is defined in custom_config\n",
    "    if 'full_file' not in config:\n",
    "        raise ValueError(\"'full_file' must be defined in the custom configuration.\")\n",
    "    \n",
    "    # Load the dataset to perform checks\n",
    "    try:\n",
    "        data = pd.read_csv(config['full_file'], sep=config['sep'])\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Failed to load the data file {config['full_file']}: {e}\")\n",
    "    \n",
    "    # Check if the required columns are present\n",
    "    missing_columns = []\n",
    "    for col in [config['sequence_column_name'], config['group_column_name']]:\n",
    "        if col not in data.columns:\n",
    "            missing_columns.append(col)\n",
    "    if missing_columns:\n",
    "        raise ValueError(f\"The following required columns are missing from the data file: {', '.join(missing_columns)}. Please check the column names or the separator and try again.\")\n",
    "\n",
    "    \n",
    "    # Check the sequence column for valid nucleotide strings\n",
    "    if not data[config['sequence_column_name']].apply(lambda x: isinstance(x, str) and all(c in 'ATCGN' for c in x)).all():\n",
    "        raise ValueError(f\"The sequence column '{config['sequence_column_name']}' contains invalid nucleotide strings.\")\n",
    "    \n",
    "    # Check the group column for less than 10 unique values\n",
    "    if data[config['group_column_name']].nunique() >= 10:\n",
    "        raise ValueError(f\"The group column '{config['group_column_name']}' must have less than 10 unique values.\")\n",
    "    \n",
    "    # Check for the length of the longest sequence\n",
    "    max_sequence_length = data[config['sequence_column_name']].str.len().max()\n",
    "    print(f\"The longest sequence is {max_sequence_length} nucleotides long.\")\n",
    "    if max_sequence_length >= 100000:\n",
    "        warnings.warn(\"Warning: The longest sequence is 100,000 or more nucleotides long. This is not a problem but can cause extremly high memory usage - check your available vram.\", RuntimeWarning)\n",
    "    elif 10000 <= max_sequence_length < 100000:\n",
    "        warnings.warn(\"Mild Warning: The longest sequence is between 10,000 and 100,000 nucleotides long. This is not a problem but can cause extensive memory usage - check your available vram.\", RuntimeWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the default configuration and the custom configuration file. Merge both to main configuration file. Create the main out directory if it is not yet present.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_main_dir(config):\n",
    "    os.makedirs(config['out_dir'], exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create the main directory of a single 'Run' based on current time. Copy the notebook inside to serve as a backup save. Split the main file to training and validation files in a 'datasets' subdirectory.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_out_dir_and_copy_source_file_in(config):\n",
    "    # Create a directory inside \"Models\" based on the current timestamp\n",
    "    current_time = datetime.datetime.now().strftime(\"%d%m%Y_%H%M%S%f\")[:18]\n",
    "    if config[\"OPTIMISE\"]:\n",
    "        directory_name = f\"{config['out_dir']}/Optimising_Run_{current_time}\"\n",
    "    else:\n",
    "        directory_name = f\"{config['out_dir']}/Individual_Run_{current_time}\"\n",
    "    os.makedirs(directory_name, exist_ok=True)\n",
    "    \n",
    "    print(f\"Out dir: {directory_name}\")\n",
    "\n",
    "    # Copy the source file to the new directory to save as a reference\n",
    "    source_file_path = \"main_guide.ipynb\"\n",
    "    destination_file_path = os.path.join(directory_name, \"main_guide.ipynb\")\n",
    "    shutil.copy(source_file_path, destination_file_path)\n",
    "    \n",
    "    return directory_name\n",
    "\n",
    "def prepare_train_test(config, directory_name):\n",
    "    \n",
    "    full_dataframe = pd.read_csv(config['full_file'], sep=config['sep'])\n",
    "    \n",
    "    train, temp = train_test_split(full_dataframe, test_size=config['test_and_validation_size']*2, stratify=full_dataframe[config['group_column_name']], random_state=42)\n",
    "    test, validation = train_test_split(temp, test_size=0.5, stratify=temp[config['group_column_name']], random_state=42)\n",
    "\n",
    "    os.makedirs(f\"{directory_name}/Created_Datasets\", exist_ok=True)\n",
    "    \n",
    "    full_csv_out = f\"{directory_name}/Created_Datasets/original_file.bed\"\n",
    "    full_dataframe.to_csv(full_csv_out, sep=config['sep'], index=False)\n",
    "    \n",
    "    training_csv_out = f\"{directory_name}/Created_Datasets/training.bed\"\n",
    "    train.to_csv(training_csv_out, sep=config['sep'], index=False)\n",
    "\n",
    "    validation_csv_out = f\"{directory_name}/Created_Datasets/validation.bed\"\n",
    "    validation.to_csv(validation_csv_out, sep=config['sep'], index=False)\n",
    "    \n",
    "    test_csv_out = f\"{directory_name}/Created_Datasets/test.bed\"\n",
    "    test.to_csv(test_csv_out, sep=config['sep'], index=False)\n",
    "    \n",
    "def prepare_the_necessary_directories_and_raw_files(config):\n",
    "    validate_config_and_data(config)\n",
    "    create_main_dir(config)\n",
    "    directory_name = create_out_dir_and_copy_source_file_in(config)\n",
    "    prepare_train_test(config, directory_name)\n",
    "    return directory_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set up the data encoding and processing functions in order to enable procedural batch generation from raw files represented as a RepeatDataset structure. The sequences and groups are proceduraly one-hot encoded and padded to the longest sequence.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_categories(group, categories_list):\n",
    "    # Convert the category to a one-hot encoded vector\n",
    "    category_index = tf.argmax(tf.equal(categories_list, group))\n",
    "    one_hot_encoded_group = tf.one_hot(category_index, depth=len(categories_list))\n",
    "    return one_hot_encoded_group\n",
    "\n",
    "def one_hot_encode(sequence, longest_sequence, encoding=['A', 'C', 'G', 'T']):\n",
    "    # Define the mapping for nucleotides to integers\n",
    "    nucleotide_to_index = tf.constant(encoding)\n",
    "    # Split the sequence into characters\n",
    "    nucleotides = tf.strings.bytes_split(sequence)\n",
    "    # Find the indices of each nucleotide in the sequence\n",
    "    indices = tf.argmax(tf.equal(tf.expand_dims(nucleotides, -1), nucleotide_to_index), axis=-1)\n",
    "    # Perform one-hot encoding\n",
    "    one_hot_encoded = tf.one_hot(indices, depth=4)\n",
    "    # Pad the sequence to the desired width\n",
    "    padded_sequence = tf.pad(one_hot_encoded, paddings=[[0, longest_sequence - tf.shape(one_hot_encoded)[0]], [0, 0]], constant_values=0)\n",
    "    # Ensure the sequence is trimmed to `longest_sequence` in case it's longer\n",
    "    padded_sequence = padded_sequence[:longest_sequence, :]\n",
    "    return padded_sequence\n",
    "\n",
    "def parse_crosslink_scores_sparse(scores_str, longest_sequence):\n",
    "    \"\"\"Parse the sparse encoded crosslink scores and create a list of scores with the given sequence length.\"\"\"\n",
    "\n",
    "    def handle_empty_or_invalid():\n",
    "        # Return a tensor of zeros with the given sequence length\n",
    "        return tf.zeros([longest_sequence], dtype=tf.float32)\n",
    "\n",
    "    def handle_valid_scores():\n",
    "        # Process the valid scores_str\n",
    "        score_pairs = tf.strings.split(scores_str, ';')\n",
    "        indices_scores = tf.strings.split(score_pairs, ':')\n",
    "        indices_scores = indices_scores.to_tensor()\n",
    "        indices = tf.strings.to_number(indices_scores[:, 0], out_type=tf.int32)\n",
    "        scores = tf.strings.to_number(indices_scores[:, 1], out_type=tf.float32)\n",
    "        \n",
    "        full_scores = tf.zeros([longest_sequence], dtype=tf.float32)\n",
    "        full_scores = tf.tensor_scatter_nd_update(full_scores, tf.expand_dims(indices, 1), scores)\n",
    "        return full_scores\n",
    "\n",
    "    # Check if the scores_str contains a \":\"\n",
    "    contains_colon = tf.strings.regex_full_match(scores_str, \".*:.*\")\n",
    "\n",
    "    return tf.cond(contains_colon, handle_valid_scores, handle_empty_or_invalid)\n",
    "\n",
    "def combined_encode(sequence, scores_str_list, longest_sequence):\n",
    "    # One-hot encode the sequence\n",
    "    one_hot_encoded_sequence = one_hot_encode(sequence, longest_sequence)\n",
    "    # Parse the crosslink scores for each clip column\n",
    "    crosslink_scores_list = [parse_crosslink_scores_sparse(scores_str, longest_sequence) for scores_str in scores_str_list]\n",
    "    # Expand the scores to have a last dimension size of 1 to match the sequence\n",
    "    crosslink_scores_expanded = [tf.expand_dims(scores, axis=-1) for scores in crosslink_scores_list]\n",
    "    # Concatenate the one-hot encoded sequence and all the scores\n",
    "    combined_encoded = tf.concat([one_hot_encoded_sequence] + crosslink_scores_expanded, axis=-1)\n",
    "    return combined_encoded\n",
    "\n",
    "def process_line(line, config, column_indices, categories_list, longest_sequence):\n",
    "    fields = tf.io.decode_csv(line, record_defaults=[[\"\"]] * len(column_indices), field_delim=config['sep'])\n",
    "    sequence = fields[column_indices[config['sequence_column_name']]]\n",
    "    groups = fields[column_indices[config['group_column_name']]]\n",
    "    scores_str_list = []\n",
    "    if config['signal_column_name']:\n",
    "        scores_str_list = [fields[column_indices[clip_column]] for clip_column in config['signal_column_name']]\n",
    "    encoded_sequence_with_scores = combined_encode(sequence, scores_str_list, longest_sequence)\n",
    "    encoded_groups = parse_categories(groups, categories_list)\n",
    "    \n",
    "    return encoded_sequence_with_scores, encoded_groups\n",
    "\n",
    "def shuffle_file(file_path, out_dir_for_scrambled_data):\n",
    "    # Specify the path to your file\n",
    "    shuffled_file_path = f\"{out_dir_for_scrambled_data}/original_file_shuffled.bed\"\n",
    "    # Read the file\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    # Separate the header and the rows\n",
    "    header = lines[0]\n",
    "    rows = lines[1:]\n",
    "    # Shuffle the rows\n",
    "    random.shuffle(rows)\n",
    "    # Write the header and the shuffled rows back to a new file\n",
    "    with open(shuffled_file_path, 'w') as file:\n",
    "        file.write(header)  # Write the header first\n",
    "        file.writelines(rows)  # Then write the shuffled rows\n",
    "    file_path = shuffled_file_path\n",
    "    return file_path\n",
    "\n",
    "def encode_from_csv(file_path, config, column_indices, longest_sequence, categories_list, out_dir_for_scrambled_data=None):\n",
    "    if out_dir_for_scrambled_data:\n",
    "        file_path = shuffle_file(file_path, out_dir_for_scrambled_data)\n",
    "            \n",
    "    dataset = tf.data.TextLineDataset(file_path).skip(1)  # Skip header\n",
    "    \n",
    "    dataset = dataset.map(lambda line: process_line(line, config, column_indices, categories_list, longest_sequence),\n",
    "                          num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    dataset = dataset.batch(config['batch_size'])\n",
    "    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prepare the information needed to encode the raw data in a RepeatDataset structure. Store the raw validation and training files in a ReturnDataset structure that contains the logic to encode the sequences and groups as needed. Get the shape of first RepeatDataset data batch. Calculate the number of steps needed to process the entire training and validation datasets.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_column_indices_max_length_and_categories(config):\n",
    "    # Read the first line to get column names\n",
    "    with tf.io.gfile.GFile(config['full_file'], 'r') as f:\n",
    "        column_names = f.readline().strip().split(config['sep'])\n",
    "        \n",
    "    column_indices = {name: index for index, name in enumerate(column_names)}\n",
    "    # Initialize to find the longest sequence and unique groups\n",
    "    longest_sequence = 0\n",
    "    unique_groups = set()\n",
    "\n",
    "    with tf.io.gfile.GFile(config['full_file'], 'r') as f:\n",
    "        next(f)  # Skip header line\n",
    "        for line in f:\n",
    "            fields = line.strip().split(config['sep'])\n",
    "            sequence = fields[column_indices[config['sequence_column_name']]]\n",
    "            group = fields[column_indices[config['group_column_name']]]\n",
    "            longest_sequence = max(longest_sequence, len(sequence))\n",
    "            unique_groups.add(group)\n",
    "\n",
    "    # Sort the unique groups to ensure consistency\n",
    "    categories_list = tf.constant(sorted(unique_groups))\n",
    "\n",
    "    return column_indices, longest_sequence, categories_list\n",
    "\n",
    "def get_shapes_of_inputs(validation_dataset):\n",
    "    # Get the first batch from the dataset\n",
    "    first_item_encoded = next(iter(validation_dataset.take(1)))\n",
    "    \n",
    "    # Extract features and labels from the first batch\n",
    "    features, labels = first_item_encoded\n",
    "    \n",
    "    # Convert TensorFlow tensors to NumPy arrays and get shapes\n",
    "    features_shape = features.numpy().shape\n",
    "    labels_shape = labels.numpy().shape\n",
    "\n",
    "    return features_shape, labels_shape\n",
    "\n",
    "def prepare_the_data_for_training(config, directory_name):\n",
    "    \n",
    "    column_indices, longest_sequence, categories_list = get_column_indices_max_length_and_categories(config)\n",
    "    \n",
    "    training_csv_out = f\"{directory_name}/Created_Datasets/training.bed\"\n",
    "    training_dataset = encode_from_csv(training_csv_out, config, column_indices, longest_sequence, categories_list).repeat()\n",
    "    \n",
    "    validation_csv_out = f\"{directory_name}/Created_Datasets/validation.bed\"\n",
    "    validation_dataset = encode_from_csv(validation_csv_out, config, column_indices, longest_sequence, categories_list).repeat()\n",
    "    \n",
    "    # Get the lengths of the CSV files without the header\n",
    "    training_length = sum(1 for _ in open(training_csv_out)) - 1\n",
    "    validation_length = sum(1 for _ in open(validation_csv_out)) - 1\n",
    "    \n",
    "    # Calculate the steps per epoch for training and validation\n",
    "    training_steps = training_length // config['batch_size'] + 1\n",
    "    validation_steps = validation_length // config['batch_size'] + 1\n",
    "    \n",
    "    # Get the shapes of the inputs\n",
    "    features_shape, labels_shape = get_shapes_of_inputs(validation_dataset)\n",
    "    return training_dataset, validation_dataset, training_steps, validation_steps, features_shape, labels_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Build the model based on the hyperparematers in a config file. The architecture can be partially customised form the custom_config file. For advanced use this can also be adapted, but it should serve as a good starting point for sequence classification tasks as is.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(config, input_shape, output_shape):\n",
    "    original_input = Input(shape=(input_shape[1], input_shape[2]))\n",
    "    x = original_input\n",
    "    \n",
    "    for block in range(config['num_blocks']):\n",
    "        x = Conv1D(filters=int(config['Final_CNN_units'] * ((1-config['CNN_Units_Increase_By_Percent']) ** (config['num_blocks']-1-block))), \n",
    "                   kernel_size=config['kernel_size']*((config['Increase_Kernel_By']) ** block), \n",
    "                   padding=\"same\", \n",
    "                   kernel_regularizer=regularizers.l2(l2=config['l2_lambda']), \n",
    "                   dilation_rate=((config['Increase_Dilation_By']) ** block)\n",
    "                   )(x)\n",
    "        \n",
    "        if config['normalization'] == 'BatchNormalization':\n",
    "            lstm_output = BatchNormalization()(lstm_output)\n",
    "        elif config['normalization'] == 'LayerNormalization':\n",
    "            lstm_output = LayerNormalization()(lstm_output)\n",
    "               \n",
    "        x = Activation('relu')(x)\n",
    "        x = Dropout(rate=config['dropout_prob'])(x)  \n",
    "        x = MaxPooling1D(pool_size=config['reduce_by'])(x)\n",
    "        \n",
    "    conv_output = x\n",
    "    \n",
    "    lstm_output = Bidirectional(GRU(units=config['LSTM_units'], \n",
    "                        return_sequences=False, \n",
    "                        kernel_regularizer=regularizers.l2(l2=config['l2_lambda']))\n",
    "                    )(conv_output)\n",
    "    \n",
    "    if config['normalization'] == 'BatchNormalization':\n",
    "        lstm_output = BatchNormalization()(lstm_output)\n",
    "    elif config['normalization'] == 'LayerNormalization':\n",
    "        lstm_output = LayerNormalization()(lstm_output)\n",
    "    \n",
    "    if config['pooling_type'] == 'average':\n",
    "        conv_output_pooled = GlobalAveragePooling1D()(conv_output)\n",
    "    elif config['pooling_type'] == 'max':\n",
    "        conv_output_pooled = GlobalMaxPooling1D()(conv_output)\n",
    "        \n",
    "    x = Concatenate()([lstm_output, conv_output_pooled])      \n",
    "    \n",
    "    \n",
    "    x = Dropout(rate=config['dropout_prob'])(conv_output_pooled)  \n",
    "    \n",
    "    for i in range(2):\n",
    "        x = Dense(units=config['Dense_units']//(2**i), \n",
    "                activation='relu', \n",
    "                kernel_regularizer=regularizers.l2(l2=config['l2_lambda'])\n",
    "                )(x)\n",
    "        \n",
    "        x = Dropout(rate=config['dropout_prob'])(x)  \n",
    "    \n",
    "    output = Dense(units=output_shape[1], activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=original_input, outputs=output)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Validate the model size to make sure the memory usage does not excede the gpu vram limit. This is very much an estimation and should not be treated as a fact.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_model_memory_usage_from_layers(config, input_shape, output_shape, dtype=np.float32):\n",
    "    \n",
    "    dtype_size = np.dtype(dtype).itemsize\n",
    "    total_weight_memory = 0\n",
    "    total_activation_memory = 0\n",
    "    \n",
    "    # Assuming the input shape includes the batch size\n",
    "    current_shape = input_shape\n",
    "    \n",
    "    # Conv1D + BatchNorm + Activation + MaxPooling for each block\n",
    "    for block in range(config['num_blocks']):\n",
    "        filters = int(config['Final_CNN_units'] * ((1-config['CNN_Units_Increase_By_Percent']) ** (config['num_blocks']-1-block)))\n",
    "        # Weight memory for Conv1D: kernel_size * current_shape[-1] * filters + filters (for bias)\n",
    "        total_weight_memory += ((config['kernel_size']*((config['Increase_Kernel_By']) ** block)) * current_shape[-1] * filters + filters) * dtype_size\n",
    "        \n",
    "        # Output shape after Conv1D\n",
    "        current_shape = (current_shape[0], current_shape[1], filters)\n",
    "        # Activation memory is for the output volume\n",
    "        total_activation_memory += np.prod(current_shape) * dtype_size\n",
    "        \n",
    "        # MaxPooling\n",
    "        current_shape = (current_shape[0], current_shape[1] // config['reduce_by'], current_shape[2])\n",
    "    \n",
    "    # LSTM Layer\n",
    "    lstm_param_count = 4 * (config['LSTM_units'] * current_shape[-1] + config['LSTM_units'] * config['LSTM_units'] + config['LSTM_units'])  # For both directions\n",
    "    total_weight_memory += lstm_param_count * dtype_size\n",
    "    if config['pooling_type'] is not None:\n",
    "        \n",
    "        # Assuming return_sequences is True, so the output shape is the same\n",
    "        total_activation_memory += np.prod((current_shape[0], current_shape[1], config['LSTM_units'] * 2)) * dtype_size\n",
    "        # Flatten\n",
    "        current_shape = (current_shape[0], np.prod(current_shape[1:]))\n",
    "    else:\n",
    "        total_activation_memory += np.prod((current_shape[0], config['LSTM_units'] * 2)) * dtype_size\n",
    "        current_shape = (current_shape[0], config['LSTM_units'] * 2)\n",
    "    \n",
    "    # Dense Layer\n",
    "    # For Dense: current_shape[-1] * Dense_units + Dense_units (for bias)\n",
    "    total_weight_memory += (current_shape[-1] * config['Dense_units'] + config['Dense_units']) * dtype_size\n",
    "    total_activation_memory += np.prod((current_shape[0], config['Dense_units'])) * dtype_size\n",
    "    \n",
    "    # Output Layer\n",
    "    total_weight_memory += (config['Dense_units'] * output_shape[1] + output_shape[1]) * dtype_size\n",
    "    total_activation_memory += np.prod((current_shape[0], output_shape[1])) * dtype_size\n",
    "    \n",
    "    total_memory_bytes = total_weight_memory + total_activation_memory\n",
    "    total_memory_mb = total_memory_bytes / (1024 ** 2)  # Convert to MB\n",
    "    \n",
    "    return total_memory_mb, total_memory_mb / 1024  # MB and GB\n",
    "\n",
    "def validate_memory_usage_before_building_the_model(config, features_shape, labels_shape):\n",
    "    # Estimate memory usage and dynamically decide the unit for display\n",
    "    memory_usage_mb, memory_usage_gb = estimate_model_memory_usage_from_layers(config, features_shape, labels_shape, dtype=np.float32)\n",
    "    if memory_usage_mb > 1024:  # More than 1 GB\n",
    "        print(f\"Estimated memory usage for a batch: {memory_usage_gb:.2f} GB\")\n",
    "    else:\n",
    "        print(f\"Estimated memory usage for a batch: {int(memory_usage_mb)} MB\")\n",
    "        \n",
    "    if memory_usage_gb > 10:\n",
    "        raise MemoryError(f\"Estimated memory usage exceeds 10 GB ({memory_usage_gb:.2f} GB). Halting execution to prevent system overload.\")\n",
    "\n",
    "        \n",
    "def validate_parameter_number_and_give_better_memory_estimation(model, features_shape):\n",
    "    \n",
    "    total_params = model.count_params()\n",
    "    if total_params > 5e7:\n",
    "        warnings.warn(\"Serious Warning: The model has over 50 million parameters. This is too much for most cases.\", RuntimeWarning)\n",
    "    elif total_params > 5e6:\n",
    "        warnings.warn(\"Warning: The model has over 5 million parameters. Consider simplifying the model if training is inefficient.\", RuntimeWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train the model based on the config file and save the training history. This can also be adapted to specific use cases.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model \n",
    "def train_the_model(config, training_dataset, validation_dataset, training_steps, validation_steps, features_shape, labels_shape):\n",
    "    \n",
    "    features_shape, labels_shape = get_shapes_of_inputs(validation_dataset)\n",
    "\n",
    "    validate_memory_usage_before_building_the_model(config, features_shape, labels_shape)\n",
    "    \n",
    "    model = build_model(config, input_shape=features_shape, output_shape=labels_shape)\n",
    "    \n",
    "    optimizer = Adam(learning_rate=config['learning_rate'])\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=[tf.keras.metrics.AUC(name='auc'), 'accuracy'])\n",
    "    \n",
    "    validate_parameter_number_and_give_better_memory_estimation(model, features_shape)\n",
    "\n",
    "    model.summary()\n",
    "    \n",
    "    # Early stopping for both training and validation losses\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=config['patience'], restore_best_weights=True)    \n",
    "    \n",
    "    verbose = 0 if config['OPTIMISE'] else 1\n",
    "        \n",
    "    # Use this dictionary in the fit method\n",
    "    history = model.fit(training_dataset,\n",
    "                        epochs=config['epochs'],\n",
    "                        verbose=verbose,\n",
    "                        steps_per_epoch=training_steps,\n",
    "                        validation_data=validation_dataset,\n",
    "                        validation_steps=validation_steps,\n",
    "                        callbacks=[early_stopping])\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prepare the validation data as a numpy array - needed for the evaluation functions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_to_numpy(dataset, steps):\n",
    "    \n",
    "    val_dataset = dataset.take(steps)\n",
    "    # To get numpy arrays from the dataset\n",
    "    features_list = []\n",
    "    labels_list = []\n",
    "\n",
    "    for features, labels in val_dataset:\n",
    "        features_list.append(features.numpy())\n",
    "        labels_list.append(labels.numpy())\n",
    "\n",
    "    X_val = np.concatenate(features_list, axis=0) if features_list else np.array([])\n",
    "    y_val = np.concatenate(labels_list, axis=0) if labels_list else np.array([])\n",
    "    \n",
    "    return X_val, y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create out directories for trained model with configuration and the evaluation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_out_dirs(directory_name, optimise):\n",
    "    # First create a general output directory\n",
    "    os.makedirs(f\"{directory_name}/Results\", exist_ok=True)\n",
    "    general_out = f\"{directory_name}/Results\"\n",
    "    if optimise:\n",
    "        model_info_out = f\"{general_out}/Saved_Trained_Model_With_Best_Parameters\"\n",
    "        os.makedirs(model_info_out, exist_ok=True)\n",
    "        model_eval_out = f\"{general_out}/Evaluated_Trained_Model_With_Best_Parameters\"\n",
    "        os.makedirs(model_eval_out, exist_ok=True)\n",
    "        optimisation_out = f\"{general_out}/Optimisation_Process_and_Results\"\n",
    "        os.makedirs(optimisation_out, exist_ok=True)\n",
    "        return model_info_out, model_eval_out, optimisation_out\n",
    "    \n",
    "    model_info_out = f\"{general_out}/Saved_Trained_Model\"\n",
    "    os.makedirs(model_info_out, exist_ok=True)\n",
    "    model_eval_out = f\"{general_out}/Evaluated_Trained_Model\"\n",
    "    os.makedirs(model_eval_out, exist_ok=True)\n",
    "    return model_info_out, model_eval_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function to predict the validation dataset. Function to evaluate and save the model's accuracy, precision, recall, F1-score, and AUROC. Function to save a confusion matrix plot and the training curves.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_validation_dataset(model, X_val, y_val, batch_size):\n",
    "    y_pred = np.vstack([model.predict(X_val[i:i + batch_size]) for i in range(0, len(X_val), batch_size)])\n",
    "    # Get the predicted classes\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    y_val_classes = np.argmax(y_val, axis=1)\n",
    "    return y_pred, y_pred_classes, y_val_classes\n",
    "\n",
    "def auroc_score(y_val, y_pred, average='standard'):\n",
    "    # Calculate ROC AUC for each class\n",
    "    auc_scores = []\n",
    "    for i in range(y_val.shape[1]):  # iterate over each class\n",
    "        # Compute ROC AUC for the i-th class\n",
    "        auc = roc_auc_score(y_val[:, i], y_pred[:, i])\n",
    "        auc_scores.append(auc)\n",
    "    if average == 'weighted':\n",
    "        # Optionally, calculate a weighted average AUROC if classes are imbalanced\n",
    "        weights = y_val.mean(axis=0)\n",
    "        weighted_average_auc = np.average(auc_scores, weights=weights)\n",
    "        return weighted_average_auc\n",
    "    else:\n",
    "        return np.average(auc_scores)\n",
    "\n",
    "def save_evaluation_metrics(y_val_classes, y_pred_classes, y_val, y_pred, directory_name):\n",
    "    metrics = {\n",
    "        \"Accuracy\": accuracy_score(y_val_classes, y_pred_classes),\n",
    "        \"Precision\": precision_score(y_val_classes, y_pred_classes, average='weighted', zero_division=0),\n",
    "        \"Recall\": recall_score(y_val_classes, y_pred_classes, average='weighted'),\n",
    "        \"F1-score\": f1_score(y_val_classes, y_pred_classes, average='weighted'),\n",
    "        \"AUROC\": auroc_score(y_val, y_pred, average='weighted')\n",
    "    }\n",
    "    with open(os.path.join(directory_name, \"evaluation_metrics.txt\"), \"w\") as f:\n",
    "        for key, value in metrics.items():\n",
    "            f.write(f\"{key}: {value}\\n\")\n",
    "\n",
    "def save_confusion_matrix(y_val_classes, y_pred_classes, directory_name):\n",
    "    # Compute the confusion matrix\n",
    "    cm = confusion_matrix(y_val_classes, y_pred_classes)\n",
    "    \n",
    "    # Normalize the confusion matrix to percentages\n",
    "    cm_percentage = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
    "    \n",
    "    # Plot and save the confusion matrix\n",
    "    plt.figure(figsize=(7, 7))\n",
    "    sns.heatmap(cm_percentage, annot=True, fmt=\".2f\", cmap=\"YlGnBu\", cbar=False)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix (Percentages)')\n",
    "    plt.savefig(os.path.join(directory_name, \"confusion_matrix.png\"))  # Save the plot as PNG\n",
    "    plt.close()  # Close the plot\n",
    "\n",
    "def save_training_curves(history, directory_name):\n",
    "    # Get the training and validation loss and accuracy values from history\n",
    "    train_loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    train_accuracy = history.history['auc']\n",
    "    val_accuracy = history.history['val_auc']\n",
    "\n",
    "    # Create a figure with two subplots for loss and accuracy\n",
    "    fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n",
    "\n",
    "    # Plot the loss curves on the first subplot\n",
    "    ax1.plot(train_loss, label='Training Loss')\n",
    "    ax1.plot(val_loss, label='Validation Loss')\n",
    "    ax1.set_title('Training and Validation Loss')\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "\n",
    "    # Plot the accuracy curves on the second subplot\n",
    "    ax2.plot(train_accuracy, label='Training AUC')\n",
    "    ax2.plot(val_accuracy, label='Validation AUC')\n",
    "    ax2.set_title('Training and Validation AUC')\n",
    "    ax2.set_xlabel('Epochs')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.legend()\n",
    "\n",
    "    # Save the figure containing both subplots\n",
    "    plt.savefig(os.path.join(directory_name, \"training_validation_curves.png\"))\n",
    "    plt.close(fig)  # Close the figure to free up memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Functions to save the model, configuration files, model summary, weights and architecture, and hyperparameters.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_training_history(history, directory_name):\n",
    "    with open(os.path.join(directory_name, \"training_history.json\"), \"w\") as f:\n",
    "        # Convert possible NumPy types in the history to native Python types\n",
    "        history_dict = {k: [float(val) for val in v] for k, v in history.history.items()}\n",
    "        json.dump(history_dict, f, indent=4)\n",
    "\n",
    "\n",
    "def save_model_summary(model, directory_name):\n",
    "    with open(os.path.join(directory_name, \"model_summary.txt\"), \"w\") as f:\n",
    "        # Redirect the default standard output to the file\n",
    "        original_stdout = sys.stdout  # Save the original standard output\n",
    "        sys.stdout = f  # Redirect to the file\n",
    "        model.summary()  # This will write to the file instead of the console\n",
    "        sys.stdout = original_stdout  # Reset standard output to its original value\n",
    "\n",
    "\n",
    "def save_weights_and_architecture(model, directory_name):\n",
    "    model.save_weights(os.path.join(directory_name, \"model_weights.h5\"))\n",
    "    with open(os.path.join(directory_name, \"model_architecture.json\"), \"w\") as f:\n",
    "        f.write(model.to_json())\n",
    "\n",
    "\n",
    "def save_models_hyperparameters(model, batch_size, directory_name):\n",
    "    with open(os.path.join(directory_name, \"hyperparameters.txt\"), \"w\") as f:\n",
    "        f.write(str(model.get_config()))\n",
    "        f.write(f\"Batch Size: {batch_size}\\n\")\n",
    "        f.write(str(model.get_config()))\n",
    "\n",
    "\n",
    "def save_full_model(model, directory_name):\n",
    "    model.save(os.path.join(directory_name, \"full_model.h5\"))\n",
    "    optimizer_config = model.optimizer.get_config()\n",
    "    with open(os.path.join(directory_name, \"config.txt\"), \"w\") as f:\n",
    "        for key, value in optimizer_config.items():\n",
    "            f.write(f\"{key}: {value}\\n\")\n",
    "\n",
    "\n",
    "def save_config(config, directory_name):\n",
    "    if config['OPTIMISE']:\n",
    "        with open(os.path.join(directory_name, \"optimal_config.json\"), \"w\") as f:\n",
    "            json.dump(config, f, indent=4)\n",
    "    else:\n",
    "        with open(os.path.join(directory_name, \"config.json\"), \"w\") as f:\n",
    "            json.dump(config, f, indent=4)        \n",
    "        \n",
    "def save_optimization_details(details, directory_name, config):\n",
    "    with open(os.path.join(directory_name, 'Optimization_Results.txt'), 'w') as f:\n",
    "        f.write(\"Optimization Results\\n\")\n",
    "        f.write(\"====================\\n\")\n",
    "        f.write(f\"Best Parameters: {details['best_params']}\\n\")\n",
    "        f.write(f\"Best Accuracy: {details['best_value']}\\n\")\n",
    "    with open(os.path.join(directory_name, 'Optimised_Config.txt'), 'w') as f:    \n",
    "        f.write(\"Optimised Config:\\n\")\n",
    "        f.write(\"====================\\n\")\n",
    "        for key, value in config.items():\n",
    "            f.write(f\"{key}: {value}\\n\")\n",
    "    with open(os.path.join(directory_name, 'Optimization_History.txt'), 'w') as f:\n",
    "        f.write(\"Optimization History:\\n\")\n",
    "        f.write(\"====================\\n\")\n",
    "        for entry in details['history']:\n",
    "            f.write(f\"Trial {entry['trial_number']}: Value: {entry['value']}, Parameters: {entry['params']}\\n\")  \n",
    "            \n",
    "                  \n",
    "def save_optimisation(directory_name, study, config):\n",
    "    # Now, save the optimization history and best parameters\n",
    "    optimization_details = {\n",
    "        'best_params': study.best_params,\n",
    "        'best_value': study.best_value,\n",
    "        'history': [{'trial_number': trial.number, 'value': trial.value, 'params': trial.params} for trial in study.trials]\n",
    "    }\n",
    "    \n",
    "    save_optimization_details(optimization_details, directory_name, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate the trained model and save it with its configuration.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_and_save_the_model(config, model, history, directory_name, validation_dataset, validation_steps, study=None):\n",
    "    # create output directory\n",
    "    model_info_out, model_eval_out, *model_optimise_out  = create_out_dirs(directory_name, config[\"OPTIMISE\"])\n",
    "    # Prepare the validation dataset for evaluation\n",
    "    X_val, y_val = dataset_to_numpy(validation_dataset, validation_steps)\n",
    "    \n",
    "    # Create the output directories\n",
    "    model_info_out, model_eval_out, *model_optimise_out  = create_out_dirs(directory_name, config[\"OPTIMISE\"])\n",
    "    \n",
    "     # Evaluate the model\n",
    "    y_pred, y_pred_classes, y_val_classes = predict_validation_dataset(model, X_val, y_val, config['batch_size'])\n",
    "    save_confusion_matrix(y_val_classes, y_pred_classes, model_eval_out)\n",
    "    save_evaluation_metrics(y_val_classes, y_pred_classes, y_val, y_pred, model_eval_out)\n",
    "    save_training_curves(history, model_eval_out)   \n",
    "    \n",
    "    # Save the model information\n",
    "    save_config(config, directory_name)\n",
    "    save_training_history(history, model_info_out)\n",
    "    save_model_summary(model, model_info_out)    \n",
    "    save_weights_and_architecture(model, model_info_out)\n",
    "    save_models_hyperparameters(model, config['batch_size'], model_info_out)\n",
    "    save_full_model(model, model_info_out)\n",
    "    if config[\"OPTIMISE\"]:\n",
    "        save_optimisation(model_optimise_out[0], study, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define the objective function for optuna optimization. Create the search space for each parameter.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, training_dataset, validation_dataset, training_steps, validation_steps, features_shape, labels_shape, config):\n",
    "    try:\n",
    "        # Define the range of hyperparameters for Optuna to explore\n",
    "        config_optuna = {\n",
    "            'batch_size': trial.suggest_categorical('batch_size', [8, 16, 32, 64]),\n",
    "            'num_blocks': trial.suggest_int('num_blocks', 2, 8),\n",
    "            'dropout_prob': trial.suggest_float('dropout_prob', 0.0, 0.4),\n",
    "            'l2_lambda': trial.suggest_loguniform('l2_lambda', 0, 1e-2),\n",
    "            'reduce_by': trial.suggest_categorical('reduce_by', [2, 3, 4]),\n",
    "            'Final_CNN_units': trial.suggest_categorical('Final_CNN_units', [16, 32, 64, 128, 256, 512, 1024]),\n",
    "            'CNN_Units_Increase_By_Percent': trial.suggest_float('CNN_Units_Increase_By_Percent', 0.1, 0.6),\n",
    "            'LSTM_units': trial.suggest_categorical('LSTM_units', [16, 32, 64, 128, 256, 512, 1024]),\n",
    "            'Dense_units': trial.suggest_categorical('Dense_units', [16, 32, 64, 128, 256, 512, 1024]),\n",
    "            'kernel_size': trial.suggest_categorical('kernel_size', [3, 5, 7, 9]),\n",
    "            'Increase_Kernel_By': trial.suggest_int('Increase_Kernel_By', 1, 4),\n",
    "            'Increase_Dilation_By': trial.suggest_int('Increase_Dilation_By', 1, 3),\n",
    "            'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-3),\n",
    "            'normalization': trial.suggest_categorical('normalization', [None, 'BatchNormalization', 'LayerNormalization']),\n",
    "            'pooling_type': trial.suggest_categorical('pooling_type', [None, 'max', 'average']),\n",
    "        }\n",
    "        \n",
    "        config_trial = {**config, **config_optuna}\n",
    "        \n",
    "        # Train the model with the trial's hyperparameters\n",
    "        model, history = train_the_model(config_trial, training_dataset, validation_dataset, training_steps, validation_steps, features_shape, labels_shape)\n",
    "\n",
    "        # Evaluate the model on the validation set to get the 'true' best performance\n",
    "        # Note: This evaluation step is crucial if early stopping might have restored the model to a state from a previous epoch\n",
    "        val_loss, val_accuracy, val_auc = model.evaluate(validation_dataset, steps=validation_steps)\n",
    "\n",
    "        # Since we're optimizing for accuracy, return the negative of accuracy (because Optuna minimizes the objective)\n",
    "        return val_accuracy\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Log the error or handle it as per your needs\n",
    "        print(f\"Error during trial: {e}\")\n",
    "        # print config_trial to see the error\n",
    "        print(config_trial)\n",
    "        # Return a very low accuracy value\n",
    "        return -1.0\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Execute optuma optimisation of the hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimise_with_optuna(config, training_dataset, validation_dataset, training_steps, validation_steps, features_shape, labels_shape):\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    \n",
    "    study.optimize(lambda trial: objective(trial, training_dataset, validation_dataset, training_steps, validation_steps, features_shape, labels_shape, config), n_trials=config[\"n_trials\"])\n",
    "\n",
    "    # Use the best hyperparameters\n",
    "    best_params = study.best_trial.params\n",
    "    best_value = study.best_trial.value\n",
    "    print(\"Best hyperparameters:\", best_params)\n",
    "    print(\"Best accuracy:\", best_value)\n",
    "    return study, best_params   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explain the model by calculating shap scores for the entire dataset, for all features and groups. Creating a file original_file_shuffled_with_contribution_scores.pkl - encoded dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_explain_out_dir(directory_name):\n",
    "    model_explanation_out = f\"{directory_name}/Results/Model_Explanation\"\n",
    "    os.makedirs(model_explanation_out, exist_ok=True)\n",
    "\n",
    "    return model_explanation_out\n",
    "\n",
    "def prepare_for_shap(config, directory_name, size_of_background = 200):\n",
    "    \n",
    "    # Number of samples\n",
    "    num_batches_needed_for_background = (size_of_background//config['batch_size']) + 1\n",
    "\n",
    "    column_indices, longest_sequence, categories_list = get_column_indices_max_length_and_categories(config)\n",
    "    full_dataset = encode_from_csv(config['full_file'], config, column_indices, longest_sequence, categories_list, out_dir_for_scrambled_data=f'{directory_name}/Created_Datasets')\n",
    "\n",
    "    full_dataset_length = sum(1 for _ in open(config['full_file'])) - 1\n",
    "\n",
    "    # Compute SHAP values for the desired number of batches\n",
    "    num_batches_needed = (full_dataset_length//config['batch_size']) + 1\n",
    "\n",
    "    # Select a small, representative background dataset\n",
    "    print('Preparing the background data...')\n",
    "    background_data, _ = dataset_to_numpy(full_dataset, num_batches_needed_for_background)\n",
    "    \n",
    "    return full_dataset, background_data, num_batches_needed\n",
    "\n",
    "def compute_shaps(config, directory_name, size_of_background):\n",
    "    full_dataset, background_data, num_batches_needed = prepare_for_shap(config, directory_name, size_of_background)\n",
    "    if config['OPTIMISE']:\n",
    "        model = load_model(f'{directory_name}/Results/Saved_Trained_Model_With_Best_Parameters/full_model.h5')\n",
    "    else:\n",
    "        model = load_model(f'{directory_name}/Results/Saved_Trained_Model/full_model.h5')\n",
    "    # Initialize the SHAP explainer (assuming a Keras model)\n",
    "    explainer = shap.GradientExplainer(model, background_data[:size_of_background], batch_size=config['batch_size'])\n",
    "    all_shap_values = []\n",
    "\n",
    "    for i, batch in tqdm(enumerate(full_dataset.take(num_batches_needed)), total=num_batches_needed, desc='Explaining Batches of Samples'):\n",
    "        \n",
    "        # To get numpy arrays from the dataset\n",
    "        features, labels = batch\n",
    "        \n",
    "        features = features.numpy()\n",
    "        labels = labels.numpy()\n",
    "        \n",
    "        # Calculate SHAP values\n",
    "        shap_values_batch = explainer.shap_values(features)\n",
    "        \n",
    "        # turn list of numpy arrays into a numpy array\n",
    "        shap_values_batch_array = np.stack(shap_values_batch)\n",
    "        all_shap_values.append(shap_values_batch_array)\n",
    "        \n",
    "    # Concatenate the SHAP values for all batches\n",
    "    all_shap_values = np.concatenate(all_shap_values, axis=1)\n",
    "    return all_shap_values\n",
    "\n",
    "def import_config(directory_name):\n",
    "\n",
    "    config_file_path = os.path.join(directory_name, 'config.json')\n",
    "    alternative_config_file_path = os.path.join(directory_name, 'optimal_config.json')\n",
    "\n",
    "    # Check which file exists and select it\n",
    "    if os.path.exists(config_file_path):\n",
    "        file_to_open = config_file_path\n",
    "    elif os.path.exists(alternative_config_file_path):\n",
    "        file_to_open = alternative_config_file_path\n",
    "    else:\n",
    "        raise FileNotFoundError(\"Neither config.json nor optimal_config.json was found.\")\n",
    "    \n",
    "    # Now open and load the configuration from the selected file\n",
    "    with open(file_to_open) as f:\n",
    "        config = json.load(f)\n",
    "        \n",
    "    return config\n",
    "\n",
    "def add_shap_values_to_full_shuffled(all_shap_values, directory_name, config, model_explanation_out):\n",
    "    full_shuffled = pd.read_csv(f'{directory_name}/Created_Datasets/original_file_shuffled.bed', sep=config['sep'])\n",
    "    unique_groups = sorted(full_shuffled[config['group_column_name']].unique())\n",
    "    for i in range(all_shap_values.shape[0]):\n",
    "        column_data = all_shap_values[i]\n",
    "\n",
    "        full_shuffled[f'Contribution_Scores_for_Group_{unique_groups[i]}'] = list(column_data)\n",
    "        \n",
    "    def adjust_array_lengths(row):\n",
    "        sequence_length = len(row[config['sequence_column_name']])\n",
    "        for i in range(len(unique_groups)):  # Assuming 3 groups, adjust as necessary\n",
    "            key = f'Contribution_Scores_for_Group_{unique_groups[i]}'\n",
    "            # Truncate the array to match sequence_length\n",
    "            # Note: This assumes you want to truncate along the first dimension\n",
    "            row[key] = row[key][:sequence_length]\n",
    "        return row\n",
    "    \n",
    "    # Apply the function across the DataFrame\n",
    "    full_shuffled = full_shuffled.apply(adjust_array_lengths, axis=1)\n",
    "    full_shuffled.to_pickle(f'{model_explanation_out}/original_file_shuffled_with_contribution_scores.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bin importance scores for a specific group over the sequnces in that group to see the importante features in classification of those seuqences.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_explain_visualisation_dir(model_explanation_out):\n",
    "    model_explanation_visualisation_out = f\"{model_explanation_out}/Visualisations\"\n",
    "    os.makedirs(model_explanation_visualisation_out, exist_ok=True)\n",
    "\n",
    "    return model_explanation_visualisation_out\n",
    "\n",
    "# Create a function that scores for kmer at each position in the sequence and then bins the sequences, sums the socres for each bin and plots them.\n",
    "def create_aggregate_heatmap_df_per_kmers(sequences, contribution_scores, explaining_params, model_explanation_out, config):\n",
    "    print(\"Creating aggregate heatmap per kmers...\")\n",
    "    # Initial empty nucleotides list\n",
    "    nucleotides = []\n",
    "    \n",
    "    # Check if sequence group names are present and append nucleotides\n",
    "    if config.get(\"sequence_column_name\", None) is not None:\n",
    "        nucleotides.extend(['A', 'C', 'G', 'T'])\n",
    "    \n",
    "    # Check if kmer length is 1 and signal columns are present\n",
    "    if explaining_params[\"kmer_length\"] == 1 and config.get(\"signal_column_name\", None) is not None:\n",
    "        # Append the score column names to the nucleotides list\n",
    "        score_column_names = config.get(\"signal_column_name\", [])\n",
    "        nucleotides.extend(score_column_names)\n",
    "    \n",
    "    # Generate all possible kmers\n",
    "    kmers = [''.join(kmer) for kmer in product(nucleotides, repeat=explaining_params[\"kmer_length\"])]\n",
    "    \n",
    "    # Create a dictionary to map nucleotides to indices\n",
    "    nucleotide_to_index = {nucleotide: idx for idx, nucleotide in enumerate(nucleotides)}\n",
    "\n",
    "    heatmap_df = pd.DataFrame()\n",
    "    \n",
    "    # For each kmer\n",
    "    for n, kmer in tqdm(enumerate(kmers), total=len(kmers), desc=\"Processing kmers\"):\n",
    "        \n",
    "        # Create empty lists for relative positions and scores\n",
    "        relative_positions_all = []\n",
    "        scores_all = []\n",
    "        sequence_index = []\n",
    "\n",
    "        # For each sequence and score\n",
    "        for num, (seq, score) in tqdm(enumerate(zip(sequences, contribution_scores)), total=len(sequences), desc=f\"Processing sequences for {kmer}\"):\n",
    "\n",
    "            # For each possible position of the kmer in the sequence\n",
    "            for j in range(len(seq) - explaining_params[\"kmer_length\"] + 1):\n",
    "\n",
    "                # Calculate the mean score for this position\n",
    "                if explaining_params[\"kmer_length\"] == 1:\n",
    "                    mean_score = score[j][nucleotide_to_index[kmer]]\n",
    "                else:\n",
    "                    mean_score = np.mean([score[j+i][nucleotide_to_index[kmer[i]]] for i in range(explaining_params[\"kmer_length\"])])\n",
    "\n",
    "                # Append the mean score and relative position to the lists\n",
    "                scores_all.append(mean_score)\n",
    "                relative_positions_all.append(j/len(seq))\n",
    "                sequence_index.append(num)\n",
    "\n",
    "        # Skip if no scores for this kmer\n",
    "        if not scores_all:\n",
    "            continue\n",
    "\n",
    "        # Create a DataFrame for easy binning\n",
    "        df = pd.DataFrame({\n",
    "            'RelativePosition': relative_positions_all,\n",
    "            f'Score_{kmer}': scores_all,\n",
    "            'Seq_Index': sequence_index\n",
    "        })\n",
    "\n",
    "        # Bin the data according to relative position\n",
    "        df['Bin'] = pd.cut(df['RelativePosition'], bins=explaining_params[\"num_bins\"], labels=False)\n",
    "\n",
    "        # Group by bin and calculate mean of scores\n",
    "        df_mean_inside_seq = df.groupby(['Seq_Index', 'Bin'])[f'Score_{kmer}'].mean().reset_index()\n",
    "\n",
    "        # Merge with the overall DataFrame\n",
    "        if heatmap_df.empty:\n",
    "            heatmap_df = df_mean_inside_seq\n",
    "        else:\n",
    "            heatmap_df = pd.merge(heatmap_df, df_mean_inside_seq, on=['Seq_Index', 'Bin'])\n",
    "    \n",
    "    # save the heatmap_df\n",
    "    heatmap_df.to_pickle(f'{model_explanation_out}/heatmap_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Turn a dataframe to an array for ease of plotting.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmar_scores_df_to_array(heatmap_df, explaining_params):\n",
    "    # Get the unique kmers\n",
    "    kmers = [col.split('_')[1] for col in heatmap_df.columns if col.startswith('Score_')]\n",
    "    # Find number of unique values in the column Bin\n",
    "    num_bins = explaining_params[\"num_bins\"]\n",
    "    \n",
    "    # Initialize the heatmap data\n",
    "    heatmap_data = np.zeros((len(kmers), num_bins))\n",
    "\n",
    "    # For each kmer\n",
    "    for n, kmer in enumerate(kmers):\n",
    "        if f'Score_{kmer}' in heatmap_df.columns:  \n",
    "            # Group by bin and calculate mean of scores for the kmer\n",
    "            df_mean_scores = heatmap_df.groupby('Bin')[f'Score_{kmer}'].mean().reset_index()\n",
    "            assert len(df_mean_scores) == num_bins , \"The clusters are so small that in some clusters we dont have all bins present. Reduce bin number or change dimensionality reduction method.\"\n",
    "\n",
    "            # Assuming bins are 0-indexed and match directly with heatmap_data columns\n",
    "            heatmap_data[n, df_mean_scores['Bin'].values] = df_mean_scores[f'Score_{kmer}'].values\n",
    "            \n",
    "    return heatmap_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot a heatmap and clustermap of binned importance scores over a group.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_a_heatmap_of_importance_scores(heatmap_df, model_explanation_visualisation_out, explaining_params, config):\n",
    "    print(\"Plotting...\")\n",
    "    nucleotides = []\n",
    "    \n",
    "    # Check if sequence group names are present and append nucleotides\n",
    "    if config.get(\"sequence_column_name\", None) is not None:\n",
    "        nucleotides.extend(['A', 'C', 'G', 'T'])\n",
    "    \n",
    "    # Check if kmer length is 1 and signal columns are present\n",
    "    if explaining_params[\"kmer_length\"] == 1 and config.get(\"signal_column_name\", None) is not None:\n",
    "        # Append the score column names to the nucleotides list\n",
    "        score_column_names = config.get(\"signal_column_name\", [])\n",
    "        nucleotides.extend(score_column_names)\n",
    "    \n",
    "    # Generate all possible kmers\n",
    "    kmers = [''.join(kmer) for kmer in product(nucleotides, repeat=explaining_params[\"kmer_length\"])]\n",
    "    \n",
    "    heatmap_data = kmar_scores_df_to_array(heatmap_df, explaining_params)\n",
    "\n",
    "    # Find the maximum value in the heatmap data\n",
    "    absolute_maximum = np.nanmax(np.abs(heatmap_data))\n",
    "    \n",
    "    # Plot\n",
    "    if explaining_params[\"kmer_length\"] == 1:\n",
    "        fig, ax = plt.subplots(figsize=(10, len(nucleotides)))\n",
    "    else:\n",
    "        fig, ax = plt.subplots(figsize=(10, 10))\n",
    "        \n",
    "    sns.heatmap(heatmap_data, cmap='seismic', cbar_kws={'label': 'Average Contribution Score'}, yticklabels=kmers, vmin=-absolute_maximum, vmax=absolute_maximum, ax=ax)\n",
    "    ax.set_xlabel('Relative position')\n",
    "    ax.set_ylabel('Kmer')\n",
    "\n",
    "    # Set x-ticks\n",
    "    x_ticks = np.linspace(0, explaining_params[\"num_bins\"], 11) \n",
    "    ax.set_xticks(x_ticks)\n",
    "    ax.set_xticklabels([f'{i*10}%' for i in range(11)])\n",
    "\n",
    "    # Set plot name and show the plot\n",
    "    plt.title(f'Metaprofile of Average Importance Scores for Each {explaining_params[\"kmer_length\"]}-mer for {explaining_params[\"group_to_explain\"]} predicted genes')\n",
    "\n",
    "    # Save plot as svg\n",
    "    plt.savefig(f'{model_explanation_visualisation_out}/{explaining_params[\"group_to_explain\"]}_group_importance_score_binned_to_{explaining_params[\"num_bins\"]}_bins_metaplot_for_{explaining_params[\"kmer_length\"]}mer.svg', bbox_inches='tight')\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "    # Plot with clustermap\n",
    "    clustermap = sns.clustermap(heatmap_data, col_cluster=False, cmap='seismic', cbar_kws={'label': 'Average Contribution Score'}, yticklabels=kmers, vmin=-absolute_maximum, vmax=absolute_maximum)\n",
    "\n",
    "    # Relabel the axes\n",
    "    clustermap.ax_heatmap.set_xlabel('Relative position')\n",
    "    clustermap.ax_heatmap.set_ylabel('Kmer')\n",
    "\n",
    "    # Set x-ticks\n",
    "    x_ticks = np.linspace(0, explaining_params[\"num_bins\"], 11)\n",
    "    clustermap.ax_heatmap.set_xticks(x_ticks)\n",
    "    clustermap.ax_heatmap.set_xticklabels([f'{i*10}%' for i in range(11)])\n",
    "\n",
    "    # Set plot name\n",
    "    clustermap.fig.suptitle(f'Metaprofile of Average Importance Scores for Each {explaining_params[\"kmer_length\"]}-mer for {explaining_params[\"group_to_explain\"]} predicted genes')\n",
    "\n",
    "    # Save plot as svg\n",
    "    plt.savefig(f'{model_explanation_visualisation_out}/{explaining_params[\"group_to_explain\"]}_group_importance_score_binned_to_{explaining_params[\"num_bins\"]}_bins_clustermap_for_{explaining_params[\"kmer_length\"]}mer.svg', bbox_inches='tight')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dimensionality_reduction(pivot_df, explaining_params):\n",
    "    if explaining_params[\"type_of_reduction\"] == 'TSNE':\n",
    "        tsne = TSNE(n_components=2, perplexity=30, learning_rate=200, random_state=42)\n",
    "        data_reduced = tsne.fit_transform(pivot_df)\n",
    "    if explaining_params[\"type_of_reduction\"] == 'UMAP':\n",
    "        reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, n_components=2, random_state=42)\n",
    "        data_reduced = reducer.fit_transform(pivot_df)\n",
    "    if explaining_params[\"type_of_reduction\"] == 'PCA':\n",
    "        pca = PCA(n_components=2)\n",
    "        data_reduced = pca.fit_transform(pivot_df)\n",
    "    if explaining_params[\"type_of_reduction\"] == 'Isomap':\n",
    "        pca = PCA(n_components=2)\n",
    "        data_reduced = pca.fit_transform(pivot_df)\n",
    "    if explaining_params[\"type_of_reduction\"] == 'Isomap':\n",
    "        isomap = Isomap(n_components=2)\n",
    "        data_reduced = isomap.fit_transform(pivot_df)\n",
    "    if explaining_params[\"type_of_reduction\"] == 'MDS':\n",
    "        mds = MDS(n_components=2)\n",
    "        data_reduced = mds.fit_transform(pivot_df)\n",
    "    if explaining_params[\"type_of_reduction\"] == 'Spectral Embedding':\n",
    "        spectral_embedding = SpectralEmbedding(n_components=2)\n",
    "        data_reduced = spectral_embedding.fit_transform(pivot_df)\n",
    "    if explaining_params[\"type_of_reduction\"] == 'Factor Analysis':\n",
    "        factor_analysis = FactorAnalysis(n_components=2)\n",
    "        data_reduced = factor_analysis.fit_transform(pivot_df)\n",
    "    if explaining_params[\"type_of_reduction\"] == 'Dictionary Learning':\n",
    "        dict_learning = DictionaryLearning(n_components=2)\n",
    "        data_reduced = dict_learning.fit_transform(pivot_df)\n",
    "        \n",
    "    return data_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clustered_heatmap_of_importance_scores(heatmap_df, model_explanation_visualisation_out, explaining_params, config): \n",
    "    print(\"Plotting...\")\n",
    "    nucleotides = []\n",
    "\n",
    "    # Check if sequence group names are present and append nucleotides\n",
    "    if config.get(\"sequence_column_name\", None) is not None:\n",
    "        nucleotides.extend(['A', 'C', 'G', 'T'])\n",
    "    \n",
    "    # Check if kmer length is 1 and signal columns are present\n",
    "    if explaining_params[\"kmer_length\"] == 1 and config.get(\"signal_column_name\", None) is not None:\n",
    "        # Append the score column names to the nucleotides list\n",
    "        score_column_names = config.get(\"signal_column_name\", [])\n",
    "        nucleotides.extend(score_column_names)\n",
    "    \n",
    "    # Generate all possible kmers\n",
    "    kmers = [''.join(kmer) for kmer in product(nucleotides, repeat=explaining_params[\"kmer_length\"])]\n",
    "    \n",
    "    features = pd.DataFrame(heatmap_df.set_index(['Seq_Index', 'Bin']).stack()).reset_index()\n",
    "    \n",
    "    features.columns = ['Seq_Index', 'Bin', 'Nucleotide', 'Score']\n",
    "    \n",
    "    # Pivot this table to have a wider format suitable for UMAP and clustering\n",
    "    pivot_df = features.pivot_table(index='Seq_Index', columns=['Bin', 'Nucleotide'], values='Score').fillna(0)\n",
    "    \n",
    "    # Perform dimensionality reduction\n",
    "    data_reduced = dimensionality_reduction(pivot_df, explaining_params)\n",
    "    \n",
    "    # Perform clustering\n",
    "    clusters_num = explaining_params[\"number_of_clusters\"]\n",
    "    aggclust = AgglomerativeClustering(n_clusters=clusters_num, distance_threshold=None, affinity='euclidean', linkage='ward')\n",
    "    clusters = aggclust.fit_predict(data_reduced)\n",
    "\n",
    "    # Add clusters back to the original DataFrame\n",
    "    pivot_df['Cluster'] = clusters\n",
    "\n",
    "    # Set up the plot\n",
    "    plt.figure(figsize=(12, 10))\n",
    "\n",
    "    # Scatter plot of the UMAP embeddings, colored by cluster assignment\n",
    "    # Note: Clusters labeled '-1' are considered outliers by HDBSCAN\n",
    "    sns.scatterplot(x=data_reduced[:, 0], y=data_reduced[:, 1], hue=clusters, palette=\"Spectral\", legend=\"full\", s=50)\n",
    "\n",
    "    plt.title(f'{explaining_params[\"type_of_reduction\"]} projection of the dataset, colored by AgglomerativeClustering clusters')\n",
    "    plt.xlabel(f'{explaining_params[\"type_of_reduction\"]} Dimension 1')\n",
    "    plt.ylabel(f'{explaining_params[\"type_of_reduction\"]} Dimension 2')\n",
    "    plt.legend(title='Cluster')\n",
    "\n",
    "    # Remove legend title for aesthetics if desired\n",
    "    plt.legend(title='Cluster').get_title().set_fontsize('14')\n",
    "\n",
    "    # Save the plot as SVG\n",
    "    plt.savefig(f'{model_explanation_visualisation_out}/{explaining_params[\"type_of_reduction\"]}_projection_of_{explaining_params[\"group_to_explain\"]}_group_importance_score_binned_to_{explaining_params[\"num_bins\"]}_bins_for_{explaining_params[\"kmer_length\"]}_clustered_into_{clusters_num}_clusters.svg', bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    # Chunk of code to add the clustering infomation to the heatmap_df\n",
    "    pivot_df_info = pivot_df.reset_index()[['Seq_Index', 'Cluster']]\n",
    "    cluster_series = pivot_df_info.iloc[:, -1]\n",
    "    clusters_info = pd.DataFrame({\n",
    "        'Seq_Index': cluster_series.index,\n",
    "        'Cluster': cluster_series.values\n",
    "    })\n",
    "    heatmap_df_with_clusters = pd.merge(heatmap_df, clusters_info, on='Seq_Index')\n",
    "    \n",
    "    # Adjust the figure size as necessary\n",
    "    fig, axs = plt.subplots(clusters_num, 1, figsize=(13, clusters_num * (4+len(score_column_names))))\n",
    "    fig.suptitle(f'Metaprofile of Average Importance Scores for Each {explaining_params[\"kmer_length\"]}-mer for {explaining_params[\"group_to_explain\"]} predicted genes', fontsize=10)\n",
    "    \n",
    "    # If there's only one cluster, wrap axs in a list to make it iterable.\n",
    "    if clusters_num == 1:\n",
    "        axs = [axs]\n",
    "        \n",
    "    absolute_maximum = 0    \n",
    "    \n",
    "    for cluster in range(clusters_num):\n",
    "        cluster_data = heatmap_df_with_clusters[heatmap_df_with_clusters['Cluster'] == cluster]\n",
    "        \n",
    "        heatmap_data = kmar_scores_df_to_array(cluster_data, explaining_params)\n",
    "\n",
    "        # Find the maximum value in the heatmap data\n",
    "        absolute_maximum = max(np.nanmax(np.abs(heatmap_data)), absolute_maximum)    \n",
    "        \n",
    "    for cluster in range(clusters_num):\n",
    "        cluster_data = heatmap_df_with_clusters[heatmap_df_with_clusters['Cluster'] == cluster]\n",
    "        \n",
    "        heatmap_data = kmar_scores_df_to_array(cluster_data, explaining_params)\n",
    "            \n",
    "        # Plotting adjustments for subplots\n",
    "        ax = axs[cluster]  # Select the current Axes object for the cluster\n",
    "        sns.heatmap(heatmap_data, cmap='seismic', cbar_kws={'label': 'Average Contribution Score'}, yticklabels=kmers, vmin=-absolute_maximum, vmax=absolute_maximum, ax=ax)\n",
    "        ax.set_title(f'Cluster {cluster}')  # Set title with cluster number\n",
    "        ax.set_xlabel('Relative position')\n",
    "        ax.set_ylabel('Kmer')\n",
    "        \n",
    "        # Set x-ticks\n",
    "        x_ticks = np.linspace(0, explaining_params[\"num_bins\"], 11) \n",
    "        ax.set_xticks(x_ticks)\n",
    "        ax.set_xticklabels([f'{i*10}%' for i in range(11)])\n",
    "\n",
    "    plt.subplots_adjust(hspace=0.4, wspace=0.2)  # Add space between plots\n",
    "    # Adjust layout and spacing\n",
    "    fig.suptitle(f'Metaprofile of Average Importance Scores for Each {explaining_params[\"kmer_length\"]}-mer for {explaining_params[\"group_to_explain\"]} predicted genes', y=1.02)\n",
    "\n",
    "    # Save the plot as SVG\n",
    "    plt.savefig(f'{model_explanation_visualisation_out}/{explaining_params[\"type_of_reduction\"]}_projection_of_{explaining_params[\"group_to_explain\"]}_group_importance_score_binned_to_{explaining_params[\"num_bins\"]}_bins_for_{explaining_params[\"kmer_length\"]}mer_clustered_into_{clusters_num}_clusters.svg', bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_importance_scores_values(shap_values, index, model_explanation_visualisation_out, explaining_params, config):\n",
    "    sequence_column_name = config.get(\"sequence_column_name\")\n",
    "    signal_column_names = config.get(\"signal_column_name\", [])\n",
    "    \n",
    "    colors = ['blue', 'orange', 'green', 'red']\n",
    "    nucleotide_mapping = ['A', 'C', 'G', 'U']\n",
    "    \n",
    "    # Get the absolute max value from the SHAP values array\n",
    "    absolute_max_value = np.abs(shap_values).max()\n",
    "\n",
    "    # Set up the figure and gridspec\n",
    "    fig = plt.figure(figsize=(16, 4+len(signal_column_names)))\n",
    "    gs = gridspec.GridSpec(2, 2, width_ratios=[50, 1], height_ratios=[4, len(signal_column_names)])\n",
    "\n",
    "    # Check if sequence column is present and plot\n",
    "    if sequence_column_name is not None:\n",
    "        ax_sequence = plt.subplot(gs[0, 0])\n",
    "        for idx in range(4):\n",
    "            ax_sequence.bar(range(shap_values.shape[0]), shap_values[:, idx], color=colors[idx], label=f'{nucleotide_mapping[idx]}', alpha=1)\n",
    "        \n",
    "        ax_sequence.set_xlim(0, shap_values.shape[0])\n",
    "        ax_sequence.set_ylim(-absolute_max_value, absolute_max_value) \n",
    "        ax_sequence.set_xlabel('Sequence position', fontsize=16)\n",
    "        ax_sequence.set_ylabel('Importance Score', fontsize=16)\n",
    "        ax_sequence.set_title(f'Importance Scores for {explaining_params[\"group_to_plot\"]} group over sequence with index {explaining_params[\"sequence_indexes\"][index]}', fontsize=16)\n",
    "        ax_sequence.legend(nucleotide_mapping, loc='upper left')\n",
    "\n",
    "    # Check if signal columns are present and plot as heatmap\n",
    "    if signal_column_names:\n",
    "        ax_heatmap = plt.subplot(gs[1, 0])\n",
    "        ax_cbar = plt.subplot(gs[1, 1])\n",
    "        num_signals = len(signal_column_names)\n",
    "        signal_data = shap_values[:, 4:4+num_signals]\n",
    "\n",
    "        sns.heatmap(signal_data.T, cmap='seismic', cbar=True, ax=ax_heatmap, cbar_ax=ax_cbar, yticklabels=signal_column_names, annot=False, vmin=-absolute_max_value, vmax=absolute_max_value)\n",
    "\n",
    "        ax_heatmap.set_xlabel('Sequence position', fontsize=16)\n",
    "        ax_heatmap.set_ylabel('Signal Columns', fontsize=16)\n",
    "        ax_heatmap.set_title(f'Signal Values for {explaining_params[\"group_to_plot\"]} group over sequence with index {explaining_params[\"sequence_indexes\"][index]}', fontsize=16)\n",
    "        ax_heatmap.set_yticklabels(signal_column_names, rotation=0, fontsize=13)  # Keep y-ticks horizontal for better readability\n",
    "\n",
    "    plt.subplots_adjust(hspace=0.5, wspace=0.2)  # Add space between plots\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{model_explanation_visualisation_out}/importance_scores_for_{explaining_params[\"group_to_plot\"]}_group_over_sequence_with_index_{explaining_params[\"sequence_indexes\"][index]}.svg', bbox_inches='tight')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_column_config(explaining_params, config):\n",
    "    # Error handling for invalid configurations\n",
    "    if explaining_params[\"kmer_length\"] > 1:\n",
    "        if config.get(\"sequence_column_name\", None) is None:\n",
    "            raise ValueError(\"There should be a sequence column present if the kmer_length is above 1.\")\n",
    "        if config.get(\"signal_column_name\", None) is not None:\n",
    "            raise ValueError(\"There should not be any score columns if the kmer_length is above 1.\")\n",
    "    elif explaining_params[\"kmer_length\"] == 1:\n",
    "        if config.get(\"sequence_column_name\", None) is None and config.get(\"signal_column_name\", None) is None:\n",
    "            raise ValueError(\"There must be at least one sequence or score column if the kmer_length is 1.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_heatmap(explaining_params):\n",
    "    directory_name = f'{explaining_params[\"run_dir\"]}'\n",
    "    config = import_config(directory_name)\n",
    "    model_explanation_out = f\"{directory_name}/Results/Model_Explanation\"\n",
    "    \n",
    "    # check if f'{model_explanation_out}/original_file_shuffled_with_contribution_scores.pkl' exists\n",
    "    if not os.path.exists(f'{model_explanation_out}/original_file_shuffled_with_contribution_scores.pkl'):\n",
    "        raise ValueError(f'{model_explanation_out}/original_file_shuffled_with_contribution_scores.pkl is prerequisite for the plotting.')\n",
    "    \n",
    "    full_shuffled_with_contribution_scores = pd.read_pickle(f'{model_explanation_out}/original_file_shuffled_with_contribution_scores.pkl')\n",
    "    \n",
    "    subset_group_shuffled_with_contribution_scores = full_shuffled_with_contribution_scores[full_shuffled_with_contribution_scores[config[\"group_column_name\"]] == explaining_params[\"group_to_explain\"]]\n",
    "    \n",
    "    if not subset_group_shuffled_with_contribution_scores[config[\"group_column_name\"]].isin([explaining_params[\"group_to_explain\"]]).any():\n",
    "        raise ValueError(f'{explaining_params[\"group_to_explain\"]} is not a value in the {config[\"group_column_name\"]} column.')\n",
    "    \n",
    "    sequences = subset_group_shuffled_with_contribution_scores[config[\"sequence_column_name\"]].tolist()\n",
    "    contribution_scores = subset_group_shuffled_with_contribution_scores[f'Contribution_Scores_for_Group_{explaining_params[\"group_to_explain\"]}'].tolist()\n",
    "    create_aggregate_heatmap_df_per_kmers(sequences, contribution_scores, explaining_params, model_explanation_out, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_heatmap(explaining_params):\n",
    "    directory_name = f'{explaining_params[\"run_dir\"]}'\n",
    "    config = import_config(directory_name)\n",
    "    check_column_config(explaining_params, config)\n",
    "    model_explanation_out = f\"{directory_name}/Results/Model_Explanation\"\n",
    "    model_explanation_visualisation_out = create_model_explain_visualisation_dir(model_explanation_out)\n",
    "\n",
    "    if not os.path.exists(f'{model_explanation_out}/heatmap_df.pkl'):\n",
    "        raise ValueError(f'{model_explanation_out}/heatmap_df.pkl is prerequisite for the plotting.')\n",
    "    \n",
    "    heatmap_df = pd.read_pickle(f'{model_explanation_out}/heatmap_df.pkl')\n",
    "    \n",
    "    if explaining_params[\"plot_heatmap_and_clustermap\"]:\n",
    "        model_explanation_visualisation_heatmap_out = f\"{model_explanation_visualisation_out}/Heatmap_and_Clustermap\"\n",
    "        os.makedirs(model_explanation_visualisation_heatmap_out, exist_ok=True)\n",
    "        plot_a_heatmap_of_importance_scores(heatmap_df, model_explanation_visualisation_heatmap_out, explaining_params, config)\n",
    "    if explaining_params[\"plot_sequences_clustered_by_bins\"]:\n",
    "        model_explanation_visualisation_heatmap_out = f\"{model_explanation_visualisation_out}/Clustered_Heatmap\"\n",
    "        os.makedirs(model_explanation_visualisation_heatmap_out, exist_ok=True)\n",
    "        plot_clustered_heatmap_of_importance_scores(heatmap_df, model_explanation_visualisation_heatmap_out, explaining_params, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_individual_sequence_importance_scores(explaining_params):\n",
    "    print(\"Plotting Individual Sequences...\")\n",
    "    directory_name = f'{explaining_params[\"run_dir\"]}'\n",
    "    config = import_config(directory_name)\n",
    "    model_explanation_out = f\"{directory_name}/Results/Model_Explanation\"\n",
    "    model_explanation_visualisation_out = create_model_explain_visualisation_dir(model_explanation_out)\n",
    "    model_explanation_visualisation_individual_sequences_out = f\"{model_explanation_visualisation_out}/Importance_Scores_Over_Sequences\"\n",
    "    os.makedirs(model_explanation_visualisation_individual_sequences_out, exist_ok=True)\n",
    "\n",
    "    if not os.path.exists(f'{model_explanation_out}/original_file_shuffled_with_contribution_scores.pkl'):\n",
    "        raise ValueError(f'{model_explanation_out}/original_file_shuffled_with_contribution_scores.pkl is prerequisite for the plotting.')\n",
    "    \n",
    "    full_shuffled_with_contribution_scores = pd.read_pickle(f'{model_explanation_out}/original_file_shuffled_with_contribution_scores.pkl')\n",
    "\n",
    "    arrays_in_index_rows = full_shuffled_with_contribution_scores.loc[explaining_params[\"sequence_indexes\"], f'Contribution_Scores_for_Group_{explaining_params[\"group_to_plot\"]}'].tolist()\n",
    "\n",
    "    for index, array_in_index_row in enumerate(arrays_in_index_rows):\n",
    "        draw_importance_scores_values(array_in_index_row, index, model_explanation_visualisation_individual_sequences_out, explaining_params, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_modisco_lite(ohe_file, attr_file, explaining_params, output_path):\n",
    "    \n",
    "    print(\"Starting TF-MoDISco execution...\")\n",
    "    \n",
    "    output_path_raw = f\"{output_path}/Raw_Output\"\n",
    "    os.makedirs(output_path_raw, exist_ok=True)  \n",
    "      \n",
    "    # Construct the command\n",
    "    output_file = f'{output_path_raw}/modisco_results_{explaining_params[\"group_to_extract\"]}_{explaining_params[\"max_seqlets\"]}_seqlets_{explaining_params[\"n_leiden\"]}_leiden_{explaining_params[\"window\"]}_window.h5'\n",
    "    max_seqlets = str(explaining_params[\"max_seqlets\"])\n",
    "    n_leiden = str(explaining_params[\"n_leiden\"])\n",
    "    window = str(explaining_params[\"window\"])\n",
    "\n",
    "    if os.path.exists(output_file):\n",
    "        return output_file\n",
    "    \n",
    "    command = [\n",
    "        'modisco', 'motifs',\n",
    "        '-s', ohe_file,\n",
    "        '-a', attr_file,\n",
    "        '-n', max_seqlets,\n",
    "        '-l', n_leiden,\n",
    "        '-w', window,\n",
    "        '-o', output_file\n",
    "    ]\n",
    "\n",
    "    # Execute the command\n",
    "    result = subprocess.run(command, capture_output=True, text=True)\n",
    "\n",
    "    # Check for errors\n",
    "    if result.returncode == 0:\n",
    "        print(\"TF-MoDISco execution successful.\")\n",
    "    else:\n",
    "        print(f\"TF-MoDISco execution failed with error: {result.stderr}\")\n",
    "    return output_file\n",
    "        \n",
    "def visualise_modisco_lite(output_file, output_path, explaining_params):\n",
    "    \n",
    "    print(\"Starting TF-MoDISco visualisation...\")\n",
    "    \n",
    "    output_path_visualised = f'{output_path}/Visualised_Output/{explaining_params[\"group_to_extract\"]}_{explaining_params[\"max_seqlets\"]}_seqlets_{explaining_params[\"n_leiden\"]}_leiden_{explaining_params[\"window\"]}_window'\n",
    "    os.makedirs(output_path_visualised, exist_ok=True)  \n",
    "    \n",
    "    # Paths to input file and output directories\n",
    "    input_file = output_file\n",
    "    output_dir = output_path_visualised\n",
    "    \n",
    "    if os.path.exists(f'{output_dir}/motifs.html'):\n",
    "        return \n",
    "\n",
    "    # Construct the command\n",
    "    command = [\n",
    "        'modisco', 'report',\n",
    "        '-i', input_file,\n",
    "        '-o', output_dir,\n",
    "        '-s', output_dir\n",
    "    ]\n",
    "\n",
    "    # Execute the command\n",
    "    result = subprocess.run(command, capture_output=True, text=True)\n",
    "\n",
    "    # Check for errors\n",
    "    if result.returncode == 0:\n",
    "        print(\"TF-MoDISco report generation successful.\")\n",
    "    else:\n",
    "        print(f\"TF-MoDISco report generation failed with error: {result.stderr}\")\n",
    "        \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_motifs(explaining_params):\n",
    "    print(\"Starting the modisco-lite motif extraction pipeline...\")\n",
    "    directory_name = f'{explaining_params[\"run_dir\"]}'\n",
    "    config = import_config(directory_name)\n",
    "    if config.get(\"sequence_column_name\", None) is None:\n",
    "        raise ValueError(\"There must be a sequence column and present as the motifs will be extracted from the sequences.\")\n",
    "    model_explanation_out = f\"{directory_name}/Results/Model_Explanation\"\n",
    "    model_explanation_visualisation_out = create_model_explain_visualisation_dir(model_explanation_out)\n",
    "    model_explanation_visualisation_modisco_out = f\"{model_explanation_visualisation_out}/TF_Modisco_Motif_Finder\"\n",
    "    os.makedirs(model_explanation_visualisation_modisco_out, exist_ok=True)\n",
    "    \n",
    "    modisco_input_path = f\"{model_explanation_visualisation_modisco_out}/TF_Modisco_Input\"\n",
    "    os.makedirs(modisco_input_path, exist_ok=True)\n",
    "    \n",
    "    modisco_one_hot_encoded_input_path = f\"{modisco_input_path}/onehot_data_for_modisco_{explaining_params['group_to_extract']}.npy\"\n",
    "    modisco_importance_scores_input_path = f\"{modisco_input_path}/importance_scores_array_for_modisco_{explaining_params['group_to_extract']}.npy\"\n",
    "    \n",
    "    original_file_shuffled_with_contribution_scores_path = f'{model_explanation_out}/original_file_shuffled_with_contribution_scores.pkl'\n",
    "\n",
    "    if not os.path.exists(original_file_shuffled_with_contribution_scores_path):\n",
    "        raise ValueError(f'{original_file_shuffled_with_contribution_scores_path} is prerequisite for the plotting.')\n",
    "    \n",
    "    full_shuffled_with_contribution_scores = pd.read_pickle(original_file_shuffled_with_contribution_scores_path)\n",
    "    full_shuffled_with_contribution_scores = full_shuffled_with_contribution_scores[full_shuffled_with_contribution_scores[config[\"group_column_name\"]] == explaining_params[\"group_to_extract\"]]\n",
    "    \n",
    "    # Extract importance scores and find the maximum sequence length\n",
    "    importance_scores = full_shuffled_with_contribution_scores[f'Contribution_Scores_for_Group_{explaining_params[\"group_to_extract\"]}'].tolist()\n",
    "    \n",
    "    max_length = max(len(seq) for seq in importance_scores)\n",
    "    \n",
    "    importance_scores = pad_sequences(importance_scores, maxlen=max_length, dtype='float32', padding='post')\n",
    "    importance_scores_array = np.array(importance_scores)\n",
    "    \n",
    "    # Truncate the importance scores to the first 4 columns as those are the seuqence columns\n",
    "    importance_scores_array = importance_scores_array[:, :, :4]\n",
    "    \n",
    "    full_shuffled_with_contribution_scores['padded_onehot'] = full_shuffled_with_contribution_scores[config[\"sequence_column_name\"]].apply(lambda x: one_hot_encode(x, max_length))\n",
    "    onehot_data_array = np.stack(full_shuffled_with_contribution_scores['padded_onehot'].values)\n",
    "    \n",
    "    #importance_scores_array = importance_scores_array * onehot_data_array\n",
    "    \n",
    "    # Apply one-hot encoding and padding to the sequences\n",
    "    if not os.path.exists(modisco_one_hot_encoded_input_path):\n",
    "        print(\"Preparing the onehot data for modisco...\")\n",
    "        onehot_data_for_modisco = np.transpose(onehot_data_array, (0, 2, 1))\n",
    "        np.save(modisco_one_hot_encoded_input_path, onehot_data_for_modisco)\n",
    "        print(onehot_data_for_modisco.shape)\n",
    "    \n",
    "    if not os.path.exists(modisco_importance_scores_input_path):    \n",
    "        print(\"Preparing the importance scores for modisco...\")\n",
    "        importance_scores_array_for_modisco = np.transpose(importance_scores_array, (0, 2, 1))\n",
    "        np.save(modisco_importance_scores_input_path, importance_scores_array_for_modisco)\n",
    "        print(importance_scores_array_for_modisco.shape)\n",
    "\n",
    "    output_file = run_modisco_lite(modisco_one_hot_encoded_input_path, modisco_importance_scores_input_path, explaining_params, model_explanation_visualisation_modisco_out)\n",
    "    visualise_modisco_lite(output_file, model_explanation_visualisation_modisco_out, explaining_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_predicitons(explaining_params):\n",
    "    directory_name = f'{explaining_params[\"run_dir\"]}'\n",
    "    config = import_config(directory_name)\n",
    "\n",
    "    column_indices, longest_sequence, categories_list = get_column_indices_max_length_and_categories(config)\n",
    "\n",
    "    full_dataset = encode_from_csv(config['full_file'], config, column_indices, longest_sequence, categories_list).repeat()\n",
    "\n",
    "    full_dataset_length = sum(1 for _ in open(config['full_file'])) - 1\n",
    "\n",
    "    # Compute SHAP values for the desired number of batches\n",
    "    num_batches_needed = (full_dataset_length // config['batch_size']) + 1\n",
    "\n",
    "    if config['OPTIMISE']:\n",
    "        model = load_model(f'{directory_name}/Results/Saved_Trained_Model_With_Best_Parameters/full_model.h5')\n",
    "    else:\n",
    "        model = load_model(f'{directory_name}/Results/Saved_Trained_Model/full_model.h5')\n",
    "\n",
    "    # Predict using the model\n",
    "    predictions = model.predict(full_dataset, steps=num_batches_needed)\n",
    "    print(\"Predictions completed.\")\n",
    "\n",
    "    # Load original dataset to append predictions\n",
    "    original_data = pd.read_csv(config['full_file'], sep=config['sep'])\n",
    "\n",
    "    # Ensure `categories_list` is a flat list of strings\n",
    "    categories_list = [cat.decode(\"utf-8\") if isinstance(cat, bytes) else cat for cat in categories_list.numpy()]\n",
    "\n",
    "    # Split predictions into separate columns based on `categories_list`\n",
    "    for i, category in enumerate(categories_list):\n",
    "        original_data[f'Prediction_{category}'] = predictions[:len(original_data), i]\n",
    "\n",
    "    original_data.to_csv(f'{directory_name}/Results/Evaluated_Trained_Model/original_file_with_predictions.bed',\n",
    "                         sep=config['sep'], index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**-- CONFIG --**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"TRAIN\": True,\n",
    "    \"out_dir\": \"/ceph/hpc/home/novljanj/data_storage/projects/ML-startup-guide/models\",\n",
    "    \"full_file\": \"/ceph/hpc/home/novljanj/data_storage/projects/semi_oops_project/Data/Naive_degs/Naive_transcripts/naive_degs_transcripts_with_fasta_good_collapsed_all_columns.bed\",\n",
    "    \"sequence_column_name\": \"merged_sequence\",\n",
    "    \"signal_column_name\": [\"clip\", \"SRSF3\", \"PABP1\"],\n",
    "    \"group_column_name\": \"category\",\n",
    "    \"sep\": \"\\t\",\n",
    "    \"test_and_validation_size\": 0.15,\n",
    "    \"batch_size\": 8,\n",
    "    \"num_blocks\": 2,\n",
    "    \"dropout_prob\": 0.1,\"l2_lambda\": 0.000,\n",
    "    \"reduce_by\": 2,\n",
    "    \"Final_CNN_units\": 100,\n",
    "    \"CNN_Units_Increase_By_Percent\": 0.5,\n",
    "    \"Increase_Kernel_By\": 1,\n",
    "    \"Increase_Dilation_By\": 1,\n",
    "    \"LSTM_units\": 50,\n",
    "    \"Dense_units\": 50,\n",
    "    \"kernel_size\": 5,\n",
    "    \"epochs\": 200,\n",
    "    \"patience\": 15,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"normalization\": None,\n",
    "    \"pooling_type\": \"max\",\n",
    "    \n",
    "    \"OPTIMISE\": False,\n",
    "    \"n_trials\": 100,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**-- TRAIN FROM SET CONFIG OR OPTIMISE THE CONFIG --**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The longest sequence is 20771 nucleotides long.\n",
      "Out dir: /ceph/hpc/home/novljanj/data_storage/projects/ML-startup-guide/models/Individual_Run_17052024_145717380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mild Warning: The longest sequence is between 10,000 and 100,000 nucleotides long. This is not a problem but can cause extensive memory usage - check your available vram.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated memory usage for a batch: 178 MB\n",
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_7 (InputLayer)        [(None, 20771, 6)]        0         \n",
      "                                                                 \n",
      " conv1d_12 (Conv1D)          (None, 20771, 50)         1550      \n",
      "                                                                 \n",
      " activation_12 (Activation)  (None, 20771, 50)         0         \n",
      "                                                                 \n",
      " dropout_30 (Dropout)        (None, 20771, 50)         0         \n",
      "                                                                 \n",
      " max_pooling1d_12 (MaxPoolin  (None, 10385, 50)        0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " conv1d_13 (Conv1D)          (None, 10385, 100)        25100     \n",
      "                                                                 \n",
      " activation_13 (Activation)  (None, 10385, 100)        0         \n",
      "                                                                 \n",
      " dropout_31 (Dropout)        (None, 10385, 100)        0         \n",
      "                                                                 \n",
      " max_pooling1d_13 (MaxPoolin  (None, 5192, 100)        0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " global_max_pooling1d_6 (Glo  (None, 100)              0         \n",
      " balMaxPooling1D)                                                \n",
      "                                                                 \n",
      " dropout_32 (Dropout)        (None, 100)               0         \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 50)                5050      \n",
      "                                                                 \n",
      " dropout_33 (Dropout)        (None, 50)                0         \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 25)                1275      \n",
      "                                                                 \n",
      " dropout_34 (Dropout)        (None, 25)                0         \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 2)                 52        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 33,027\n",
      "Trainable params: 33,027\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "80/80 [==============================] - 3s 16ms/step - loss: 0.7031 - auc: 0.5128 - accuracy: 0.5071 - val_loss: 0.6895 - val_auc: 0.6354 - val_accuracy: 0.4964\n",
      "Epoch 2/200\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.6932 - auc: 0.5326 - accuracy: 0.5433 - val_loss: 0.6862 - val_auc: 0.6453 - val_accuracy: 0.5182\n",
      "Epoch 3/200\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.6850 - auc: 0.5738 - accuracy: 0.5559 - val_loss: 0.6770 - val_auc: 0.6622 - val_accuracy: 0.5401\n",
      "Epoch 4/200\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.6500 - auc: 0.6805 - accuracy: 0.6331 - val_loss: 0.6463 - val_auc: 0.7395 - val_accuracy: 0.6642\n",
      "Epoch 5/200\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.6247 - auc: 0.7179 - accuracy: 0.6646 - val_loss: 0.5990 - val_auc: 0.7802 - val_accuracy: 0.7299\n",
      "Epoch 6/200\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.5727 - auc: 0.7763 - accuracy: 0.7181 - val_loss: 0.6145 - val_auc: 0.7200 - val_accuracy: 0.6350\n",
      "Epoch 7/200\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.5137 - auc: 0.8276 - accuracy: 0.7606 - val_loss: 0.5790 - val_auc: 0.7665 - val_accuracy: 0.7007\n",
      "Epoch 8/200\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.4747 - auc: 0.8574 - accuracy: 0.7780 - val_loss: 0.5467 - val_auc: 0.7962 - val_accuracy: 0.7518\n",
      "Epoch 9/200\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.4268 - auc: 0.8857 - accuracy: 0.8094 - val_loss: 0.6000 - val_auc: 0.7663 - val_accuracy: 0.6788\n",
      "Epoch 10/200\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.3913 - auc: 0.9060 - accuracy: 0.8425 - val_loss: 0.5558 - val_auc: 0.7992 - val_accuracy: 0.7518\n",
      "Epoch 11/200\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.3619 - auc: 0.9196 - accuracy: 0.8472 - val_loss: 0.5692 - val_auc: 0.7919 - val_accuracy: 0.7664\n",
      "Epoch 12/200\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.3353 - auc: 0.9326 - accuracy: 0.8520 - val_loss: 0.5957 - val_auc: 0.7942 - val_accuracy: 0.7445\n",
      "Epoch 13/200\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.3114 - auc: 0.9415 - accuracy: 0.8661 - val_loss: 0.6909 - val_auc: 0.7624 - val_accuracy: 0.6934\n",
      "Epoch 14/200\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2561 - auc: 0.9586 - accuracy: 0.9150 - val_loss: 0.6575 - val_auc: 0.7835 - val_accuracy: 0.7372\n",
      "Epoch 15/200\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2347 - auc: 0.9667 - accuracy: 0.9055 - val_loss: 0.6936 - val_auc: 0.7804 - val_accuracy: 0.7372\n",
      "Epoch 16/200\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2348 - auc: 0.9660 - accuracy: 0.9102 - val_loss: 0.7237 - val_auc: 0.7794 - val_accuracy: 0.7445\n",
      "Epoch 17/200\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2853 - auc: 0.9501 - accuracy: 0.8787 - val_loss: 0.6795 - val_auc: 0.7820 - val_accuracy: 0.7372\n",
      "Epoch 18/200\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2272 - auc: 0.9690 - accuracy: 0.9118 - val_loss: 0.7136 - val_auc: 0.7753 - val_accuracy: 0.7007\n",
      "Epoch 19/200\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2110 - auc: 0.9728 - accuracy: 0.9118 - val_loss: 0.6881 - val_auc: 0.7811 - val_accuracy: 0.7299\n",
      "Epoch 20/200\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2817 - auc: 0.9534 - accuracy: 0.8850 - val_loss: 0.5985 - val_auc: 0.7832 - val_accuracy: 0.7372\n",
      "Epoch 21/200\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2283 - auc: 0.9686 - accuracy: 0.9087 - val_loss: 0.6564 - val_auc: 0.7808 - val_accuracy: 0.7226\n",
      "Epoch 22/200\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.2085 - auc: 0.9742 - accuracy: 0.9102 - val_loss: 0.6708 - val_auc: 0.7841 - val_accuracy: 0.7299\n",
      "Epoch 23/200\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.1934 - auc: 0.9769 - accuracy: 0.9228 - val_loss: 0.7454 - val_auc: 0.7646 - val_accuracy: 0.7226\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n"
     ]
    }
   ],
   "source": [
    "def train_or_optimise(config):\n",
    "    directory_name = prepare_the_necessary_directories_and_raw_files(config)\n",
    "    training_dataset, validation_dataset, training_steps, validation_steps, features_shape, labels_shape = prepare_the_data_for_training(config, directory_name)\n",
    "    \n",
    "    study = None\n",
    "    if config['OPTIMISE']:\n",
    "        study, best_params = optimise_with_optuna(config, training_dataset, validation_dataset, training_steps, validation_steps, features_shape, labels_shape)\n",
    "        config.update(best_params)\n",
    "        \n",
    "    model, history = train_the_model(config, training_dataset, validation_dataset, training_steps, validation_steps, features_shape, labels_shape)\n",
    "    evaluate_and_save_the_model(config, model, history, directory_name, validation_dataset, validation_steps, study)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    (config['TRAIN'] or config['OPTIMISE']) and train_or_optimise(config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**-- CALCULATE THE IMPORTANCE SCORES FOR A GIVEN RUN AND/OR PLOT THEM--**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the modisco-lite motif extraction pipeline...\n",
      "Starting TF-MoDISco execution...\n",
      "TF-MoDISco execution failed with error: Traceback (most recent call last):\n",
      "  File \"/ceph/hpc/home/novljanj/.conda/envs/tf/bin/modisco\", line 130, in <module>\n",
      "    pos_patterns, neg_patterns = modiscolite.tfmodisco.TFMoDISco(\n",
      "  File \"/ceph/hpc/home/novljanj/.conda/envs/tf/lib/python3.9/site-packages/modiscolite/tfmodisco.py\", line 339, in TFMoDISco\n",
      "    neg_patterns = seqlets_to_patterns(seqlets=neg_seqlets,\n",
      "  File \"/ceph/hpc/home/novljanj/.conda/envs/tf/lib/python3.9/site-packages/modiscolite/tfmodisco.py\", line 208, in seqlets_to_patterns\n",
      "    cluster_indices = cluster.LeidenCluster(\n",
      "  File \"/ceph/hpc/home/novljanj/.conda/envs/tf/lib/python3.9/site-packages/modiscolite/cluster.py\", line 22, in LeidenCluster\n",
      "    partition = leidenalg.find_partition(\n",
      "  File \"/ceph/hpc/home/novljanj/.conda/envs/tf/lib/python3.9/site-packages/leidenalg/functions.py\", line 81, in find_partition\n",
      "    partition = partition_type(graph,\n",
      "  File \"/ceph/hpc/home/novljanj/.conda/envs/tf/lib/python3.9/site-packages/leidenalg/VertexPartition.py\", line 456, in __init__\n",
      "    self._partition = _c_leiden._new_ModularityVertexPartition(pygraph_t,\n",
      "BaseException: Could not construct partition: Cannot accept NaN weights.\n",
      "\n",
      "Starting TF-MoDISco visualisation...\n",
      "TF-MoDISco report generation failed with error: Traceback (most recent call last):\n",
      "  File \"/ceph/hpc/home/novljanj/.conda/envs/tf/bin/modisco\", line 146, in <module>\n",
      "    modiscolite.report.report_motifs(args.h5py, args.output, img_path_suffix=args.suffix, meme_motif_db=args.meme_db, \n",
      "  File \"/ceph/hpc/home/novljanj/.conda/envs/tf/lib/python3.9/site-packages/modiscolite/report.py\", line 236, in report_motifs\n",
      "    create_modisco_logos(modisco_h5py, modisco_logo_dir, trim_threshold, pattern_groups)\n",
      "  File \"/ceph/hpc/home/novljanj/.conda/envs/tf/lib/python3.9/site-packages/modiscolite/report.py\", line 185, in create_modisco_logos\n",
      "    modisco_results = h5py.File(modisco_h5py, 'r')\n",
      "  File \"/ceph/hpc/home/novljanj/.conda/envs/tf/lib/python3.9/site-packages/h5py/_hl/files.py\", line 567, in __init__\n",
      "    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n",
      "  File \"/ceph/hpc/home/novljanj/.conda/envs/tf/lib/python3.9/site-packages/h5py/_hl/files.py\", line 231, in make_fid\n",
      "    fid = h5f.open(name, flags, fapl=fapl)\n",
      "  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
      "  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
      "  File \"h5py/h5f.pyx\", line 106, in h5py.h5f.open\n",
      "FileNotFoundError: [Errno 2] Unable to open file (unable to open file: name = '/ceph/hpc/home/novljanj/data_storage/projects/ML-startup-guide/models/Individual_Run_17052024_145717380/Results/Model_Explanation/Visualisations/TF_Modisco_Motif_Finder/Raw_Output/modisco_results_OOPS & Semi-extractibilty DEGsn = 456_4000_seqlets_2_leiden_400_window.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "explaining_params = {\n",
    "\n",
    "    \"EXPLAIN\": False, # Set to false if importance scores are already computed\n",
    "    \"run_dir\": '/ceph/hpc/home/novljanj/data_storage/projects/ML-startup-guide/models/Individual_Run_17052024_145717380', # The output directory for the whole training run\n",
    "    \"num_background_samples\": 200, # The number of samples to use for background in SHAP\n",
    "    \n",
    "    \"ADD_PREDICTIONS\": False, # Add the prediction for each sequence to the original file\n",
    "    \n",
    "    \"PLOT_IMPORTANCE_SCORES_OVER_INDIVIDUAL_SEQUENCES\": False, # Set to false if binned importance scores are already computed\n",
    "    \"group_to_plot\": \"OOPS & Semi-extractibilty DEGsn = 456\", # The group to explain\n",
    "    \"sequence_indexes\": [0], # Index of the sequence to plot from the original_file_shuffled, takes also lists of indexes\n",
    "    \n",
    "    \"BIN_IMPORTANCE_SCORES\": False, # Set to false if binned importance scores are already computed\n",
    "    \"group_to_explain\": \"OOPS & Semi-extractibilty DEGsn = 456\", # The group to explain\n",
    "    \"kmer_length\": 1, # The kmer to analyse 1=Nucleotide, 2=Dinucleotide, 3=Trinucleotide ...\n",
    "    \"num_bins\": 10, # Make sure this is not much longer then the smallest sequences\n",
    "    \"plot_heatmap_and_clustermap\": False, # Set to false if you don't want to plot the clustermap\n",
    "    \"plot_sequences_clustered_by_bins\": False, # Set to false if you don't want to plot the sequences clustered based on their importance scores\n",
    "    \"type_of_reduction\": \"UMAP\", # Choose from TSNE, UMAP, PCA, Isomap, MDS, Spectral Embedding, Factor Analysis, Dictionary Learning\n",
    "    \"number_of_clusters\": 3, # Number of clusters formed after dim reduction\n",
    "    \n",
    "    \"EXTRACT_MOTIFS_IN_IMPORTANCE_SCORES\": True, # Set to false if binned importance scores are already computed\n",
    "    \"group_to_extract\": \"OOPS & Semi-extractibilty DEGsn = 456\", # The group to explain\n",
    "    \"max_seqlets\": 4000, # The maximum number of seqlets per metacluster.\n",
    "    \"n_leiden\": 2, # The number of Leiden clusterings to perform with different random seeds.\n",
    "    \"window\": 400, # The window surrounding the peak center that will be considered for motif discovery.\n",
    "}\n",
    "\n",
    "def explain_run(explaining_params):\n",
    "    directory_name = f'{explaining_params[\"run_dir\"]}'\n",
    "    model_explanation_out = create_model_explain_out_dir(directory_name)\n",
    "    config = import_config(directory_name)\n",
    "    all_shap_values = compute_shaps(config, directory_name, explaining_params[\"num_background_samples\"])\n",
    "    add_shap_values_to_full_shuffled(all_shap_values, directory_name, config, model_explanation_out)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    explaining_params[\"ADD_PREDICTIONS\"] and add_predicitons(explaining_params)\n",
    "    explaining_params[\"EXPLAIN\"] and explain_run(explaining_params)\n",
    "    explaining_params[\"BIN_IMPORTANCE_SCORES\"] and create_heatmap(explaining_params)\n",
    "    (explaining_params[\"plot_heatmap_and_clustermap\"] or  explaining_params[\"plot_sequences_clustered_by_bins\"]) and plot_heatmap(explaining_params)\n",
    "    explaining_params[\"PLOT_IMPORTANCE_SCORES_OVER_INDIVIDUAL_SEQUENCES\"] and plot_individual_sequence_importance_scores(explaining_params)\n",
    "    explaining_params[\"EXTRACT_MOTIFS_IN_IMPORTANCE_SCORES\"] and extract_motifs(explaining_params)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import a .npy file and check for NaN values\n",
    "test = np.isnan(np.load('/ceph/hpc/home/novljanj/data_storage/projects/ML-startup-guide/models/Individual_Run_17052024_145717380/Results/Model_Explanation/Visualisations/TF_Modisco_Motif_Finder/TF_Modisco_Input/importance_scores_array_for_modisco_OOPS & Semi-extractibilty DEGsn = 456.npy')).any()\n",
    "test\n",
    "# import a .npy file and check for NaN values\n",
    "test = np.isnan(np.load('/ceph/hpc/home/novljanj/data_storage/projects/ML-startup-guide/models/Individual_Run_17052024_145717380/Results/Model_Explanation/Visualisations/TF_Modisco_Motif_Finder/TF_Modisco_Input/onehot_data_for_modisco_OOPS & Semi-extractibilty DEGsn = 456.npy')).any()\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 1., 1., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 1., ..., 0., 0., 0.],\n",
       "        [1., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 1., 1., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[1., 0., 1., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 1., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[1., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 1., 1., ..., 0., 0., 0.]]], dtype=float32)"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = np.load('/ceph/hpc/home/novljanj/data_storage/projects/ML-startup-guide/models/Individual_Run_17052024_145717380/Results/Model_Explanation/Visualisations/TF_Modisco_Motif_Finder/TF_Modisco_Input/onehot_data_for_modisco_OOPS & Semi-extractibilty DEGsn = 456.npy')\n",
    "test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
